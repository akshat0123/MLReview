{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spiritual-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-tissue",
   "metadata": {},
   "source": [
    "---\n",
    "slug: \"/blog/optimizationfordeeplearning\"\n",
    "date: \"2021-04-12\"\n",
    "title: \"Optimization for Deep Learning\"\n",
    "category: \"2 Deep Learning\"\n",
    "order: 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-thunder",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-filter",
   "metadata": {},
   "source": [
    "Optimization is the process used to minimize the loss function during the training process of a neural network.\n",
    "There are a variety of different approaches to optimization.\n",
    "This post will discuss some of those approaches, including gradient descent, stochastic gradient descent, RMSProp, and Adam.\n",
    "This post will also cover generalized optimization practices, such as momentum as well as adaptive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-chick",
   "metadata": {},
   "source": [
    "### Basic Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-metro",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-checkout",
   "metadata": {},
   "source": [
    "Vanilla gradient descent is one of the simplest approaches to optimization.\n",
    "The general process is to reduce the loss function by moving the weights in the opposite direction of the gradient.\n",
    "The weight update performed in gradient descent is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-recipient",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    w &= w - \\alpha \\frac{\\partial L}{\\partial w}&\n",
    "    \\text{Weight update}\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter} &\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-repeat",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-brazilian",
   "metadata": {},
   "source": [
    "Stochastic gradient descent is a variant of gradient descent and is one of the most popular optimization techniques in machine learning.\n",
    "The basic difference between stochastic gradient descent and plain gradient descent is that in stochastic gradient descent, weights are updated after a single randomly-drawn point of data is seen (or a randomly-drawn batch of data points), rather than only when the entire dataset has been seen, as in plain gradient descent.\n",
    "Minibatch stochastic gradient descent is a variant of stochastic gradient descent where the weights of a network are updated using a batch of randomly-drawn input data rather than a single data point.\n",
    "The weight updates for stochastic gradient descent and minibatch stochastic gradient descent are shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-chapter",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    w &= w - \\alpha \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "    &\\text{Weight update}\\\\\n",
    "    w &= w - \\alpha \\left[\\frac{1}{B}\\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i;w))}{\\partial w}\\right]\n",
    "    &\\text{Minibatch weight update}\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter} &\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-albania",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-apple",
   "metadata": {},
   "source": [
    "Momentum is a genralizable method to accelerate optimization that uses a moving average of past gradients to update weights rather than simply the last calculated gradient.\n",
    "The exponentially decaying moving average of gradients, $v$, is the velocity at which the weights of the model move.\n",
    "Nesterov momentum is a momentum method variant that evaluates the gradient after applying the current velocity, rather than before, as in standard momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-sapphire",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    v &= \\epsilon v - \\alpha \\left[\\frac{1}{B}\\sum^B_{i=1}\\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\\right]\n",
    "    & \\text{SGD with Momentum}\\\\\n",
    "    v &= \\epsilon v - \\alpha \\left[\\frac{1}{B}\\sum^B_{i=1}\\frac{\\partial L(y_i, f(x_i; w + \\epsilon v))}{\\partial w}\\right]\n",
    "    & \\text{SGD with Nesterov Momentum}\\\\\n",
    "    w &= w + v \n",
    "    &\\text{Weight update}\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter}&\\\\\n",
    "    \\epsilon &\\rightarrow \\text{model hyperparameter}&\\\\\n",
    "    v & \\rightarrow \\text{velocity}&\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-hardwood",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-simon",
   "metadata": {},
   "source": [
    "The code below displays an `Optimizer` class its subclasses for stochastic gradient descent and stochastic gradient descent with momentum.\n",
    "These classes are meant to be used in conjunction with the `Network` and `Layer` classes I describe in my earlier post, titled *Classification and Regression with Neural Networks*.\n",
    "The code for `Network` and `Layer` to be used with the `Optimizer` class can be found in the following package from my github page [here](https://github.com/akshat0123/MLReview/tree/main/Packages/mlr/NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    \"\"\" Abstract base class for optimizers\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize optimizer\n",
    "        \"\"\"\n",
    "        pass            \n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def __copy__(self):\n",
    "        \"\"\" Copy class instance\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" Update weights\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "def SGDOptimizer(momentum: bool=False, epsilon: float=1e-4) -> Optimizer:\n",
    "    \"\"\" Return stochastic gradient descent optimizer\n",
    "\n",
    "    Args:\n",
    "        momentum: whether to include momentum or not\n",
    "        epsilon: epsilon parameter for momentum\n",
    "\n",
    "    Returns:\n",
    "        stochastic gradient descent optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = SGDMomentumOptimizer(epsilon=epsilon) if momentum else DefaultSGDOptimizer()\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "class DefaultSGDOptimizer(Optimizer):\n",
    "    \"\"\" Stochastic Gradient Descent optimizer (without momentum)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\" Initialize default SGD optimizer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def __copy__(self):\n",
    "        \"\"\" Return copy of default SGD optimizer\n",
    "\n",
    "        Returns: \n",
    "            copy of optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        return DefaultSGDOptimizer()\n",
    "\n",
    "\n",
    "    def update(self, w: torch.Tensor, alpha: float, dw: torch.Tensor, dr: torch.Tensor, lambdaa: float=1.0) -> torch.Tensor:\n",
    "        \"\"\" Update weights\n",
    "\n",
    "        Args:\n",
    "            w: weight tensor\n",
    "            alpha: learning rate \n",
    "            dw: weight gradient\n",
    "            dr: regularization gradient\n",
    "            lambdaa: regularization lambda parameter\n",
    "\n",
    "        Returns: \n",
    "            updated weight tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return w - (alpha * (dw + (lambdaa * dr)))\n",
    "\n",
    "\n",
    "class SGDMomentumOptimizer(Optimizer):\n",
    "    \"\"\" Stochastic Gradient Descent optimizer (with momentum)\n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    def __init__(self, epsilon: float=1e-4) -> None:\n",
    "        \"\"\" Initialize default SGD optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.v = None\n",
    "\n",
    "\n",
    "    def __copy__(self):\n",
    "        \"\"\" Return copy of default SGD optimizer\n",
    "\n",
    "        Returns: \n",
    "            copy of optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        return SGDMomentumOptimizer(epsilon=self.epsilon)            \n",
    "\n",
    "\n",
    "    def update(self, w: torch.Tensor, alpha: float, dw: torch.Tensor, dr: torch.Tensor, lambdaa: float=1.0) -> torch.Tensor:\n",
    "        \"\"\" Update weights\n",
    "\n",
    "        Args:\n",
    "            w: weight tensor\n",
    "            alpha: learning rate \n",
    "            dw: weight gradient\n",
    "            dr: regularization gradient\n",
    "            lambdaa: regularization lambda parameter\n",
    "\n",
    "        Returns: \n",
    "            updated weight tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self.v is None: \n",
    "            self.v = torch.zeros(w.shape)\n",
    "\n",
    "        self.v = (self.epsilon * self.v) - (alpha * (dw + (lambdaa * dr)))\n",
    "        return w + self.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-marsh",
   "metadata": {},
   "source": [
    "### Adaptive Learning Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-yacht",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-distance",
   "metadata": {},
   "source": [
    "AdaGrad is an optimization approach that has a learning rate for all parameters ($r$ in the equation below), and updates the learning rates continuously.\n",
    "Learning rates are decayed proportionally with regard to how often updates are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-vertical",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    r &= r + \\left[\\frac{1}{B}\\sum^B_{i=1}\\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\\right]^2\\\\\n",
    "    w &= w - \\frac{\\alpha}{\\delta + \\sqrt{r}} \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "     \\right]\\\\\n",
    "     \\alpha &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "     \\delta &\\rightarrow \\text{small constant, usually }10^{-6}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-thanksgiving",
   "metadata": {},
   "source": [
    "#### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-premises",
   "metadata": {},
   "source": [
    "RMSProp is an optimization approah that also has a learning rate for all parameters ($r$ in the equation below), and updates the learning rates continuously.\n",
    "When compared to AdaGrad, learning rates in RMSProp do not diminish nearly as fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-ethiopia",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    r &= \\rho r + (1-\\rho) \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "     \\right]^2\\\\\n",
    "     w &= w - \\frac{\\alpha}{\\sqrt{\\delta + r}} \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "     \\right]\\\\\n",
    "     \\alpha &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "     \\rho &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "     \\delta &\\rightarrow \\text{small constant, usually }10^{-6}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-garlic",
   "metadata": {},
   "source": [
    "#### RMSProp with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-sharing",
   "metadata": {},
   "source": [
    "Momentum can also be added to the RMSProp optimization method as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-administration",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    r &= \\rho r + (1-\\rho) \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "     \\right]^2\\\\\n",
    "    v &= \\epsilon v - \\frac{\\alpha}{\\sqrt{\\delta + r}} \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "     \\right]\\\\\n",
    "    w &= w + v\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\rho &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\delta &\\rightarrow \\text{small constant, usually }10^{-6}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-profession",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-exhaust",
   "metadata": {},
   "source": [
    "Adam is an adaptive learning optimization method that limits the quick diminishing of learning rates, similar to RMSProp.\n",
    "In addition, unlike RMSProp or AdaGrad, Adam also keeps a decaying average of nonsquared gradients ($s$ in the equation below), which can be seen to serve a purpose similar to momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-tobacco",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    s &= \\rho_1 s + (1 - \\rho_1) \\left[ \n",
    "        \\frac{1}{B} \\sum^B_{i=1} \n",
    "        \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w} \n",
    "    \\right]\\\\\n",
    "    r &= \\rho_2 r + (1 - \\rho_2) \\left[ \n",
    "        \\frac{1}{B} \\sum^B_{i=1} \n",
    "        \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w} \n",
    "    \\right]^2\\\\\n",
    "    \\hat{s} &= \\frac{s}{1-\\rho_1}\\\\\n",
    "    \\hat{r} &= \\frac{r}{1-\\rho_2}\\\\\n",
    "    w &= w - \\alpha \\left[ \n",
    "        \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}\n",
    "    \\right]\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\rho_1 &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\rho_2 &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\delta &\\rightarrow \\text{small constant, usually }10^{-6}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-allergy",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-philippines",
   "metadata": {},
   "source": [
    "The code for AdaGrad, RMSProp, and Adam optimizers is displayed in the block below.\n",
    "These classes are meant to be used in conjunction with the `Network` and `Layer` classes I describe in my earlier post, titled *Classification and Regression with Neural Networks*.\n",
    "The code for `Network` and `Layer` to be used with the `Optimizer` class can be found in the following package from my github page [here](https://github.com/akshat0123/MLReview/tree/main/Packages/mlr/NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGradOptimizer(Optimizer):\n",
    "    \"\"\" AdaGrad optimizer\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\" Initialize AdaGrad optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        self.delta = 1e-5 \n",
    "        self.r = None\n",
    "\n",
    "\n",
    "    def __copy__(self):\n",
    "        \"\"\" Return copy of default SGD optimizer\n",
    "\n",
    "        Returns: \n",
    "            copy of optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        return AdaGradOptimizer()\n",
    "\n",
    "\n",
    "    def update(self, w: torch.Tensor, alpha: float, dw: torch.Tensor, dr: torch.Tensor, lambdaa: float=1.0) -> torch.Tensor:\n",
    "        \"\"\" Update weights\n",
    "\n",
    "        Args:\n",
    "            w: weight tensor\n",
    "            alpha: learning rate \n",
    "            dw: weight gradient\n",
    "            dr: regularization gradient\n",
    "            lambdaa: regularization lambda parameter\n",
    "\n",
    "        Returns: \n",
    "            updated weight tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self.r is None: \n",
    "            self.r = torch.zeros(w.shape)\n",
    "\n",
    "        self.r = self.r + (dw)**2\n",
    "        return w - (alpha * ((dw + (lambdaa * dr)) / (self.delta + torch.sqrt(self.r))))\n",
    "\n",
    "\n",
    "def RMSPropOptimizer(momentum: bool=False, rho: float=0.9, epsilon: float=1e-4) -> Optimizer:\n",
    "    \"\"\" Return RMSProp optimizer\n",
    "\n",
    "    Args:\n",
    "        momentum: whether to include momentum or not\n",
    "        epsilon: epsilon parameter for momentum\n",
    "        rho: rho parameter for RMSProp\n",
    "\n",
    "    Returns:\n",
    "        RMSProp optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = RMSPropMomentumOptimizer(rho=rho, epsilon=epsilon) if momentum else DefaultRMSPropOptimizer(rho=rho)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "class DefaultRMSPropOptimizer:\n",
    "    \"\"\" RMSProp optimizer (without momentum)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, rho: float=0.9) -> None:\n",
    "        \"\"\" Initialize optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        self.delta = 1e-5\n",
    "        self.rho = rho\n",
    "        self.r = None\n",
    "\n",
    "\n",
    "    def __copy__(self):\n",
    "        \"\"\" Return copy of DefaultRMSPropOptimizer\n",
    "        \"\"\"\n",
    "\n",
    "        return DefaultRMSPropOptimizer(self.rho)\n",
    "\n",
    "\n",
    "    def update(self, w: torch.Tensor, alpha: float, dw: torch.Tensor, dr: torch.Tensor, lambdaa: float=1.0) -> torch.Tensor:\n",
    "        \"\"\" Update weights\n",
    "\n",
    "        Args:\n",
    "            w: weight tensor\n",
    "            alpha: learning rate \n",
    "            dw: weight gradient\n",
    "            dr: regularization gradient\n",
    "            lambdaa: regularization lambda parameter\n",
    "\n",
    "        Returns: \n",
    "            updated weight tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self.r is None:\n",
    "            self.r = torch.zeros(w.shape)\n",
    "\n",
    "        self.r = (self.rho * self.r) + ((1 - self.rho) * (dw**2))\n",
    "        return w - (alpha * ((dw + (lambdaa * dr)) / (torch.sqrt(self.delta + self.r))))\n",
    "\n",
    "\n",
    "class RMSPropMomentumOptimizer:\n",
    "    \"\"\" RMSProp optimizer (without momentum)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, rho: float=0.9, epsilon: float=1e-4):\n",
    "        \"\"\" Initialize optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = 1e-05\n",
    "        self.rho = rho\n",
    "        self.r = None\n",
    "        self.v = None\n",
    "\n",
    "\n",
    "    def __copy__(self):\n",
    "        \"\"\" Return copy of RMSPropMomentumOptimizer\n",
    "        \"\"\"\n",
    "\n",
    "        return RMSPropMomentumOptimizer(self.rho, self.epsilon)\n",
    "\n",
    "\n",
    "    def update(self, w: torch.Tensor, alpha: float, dw: torch.Tensor, dr: torch.Tensor, lambdaa: float=1.0) -> torch.Tensor:\n",
    "        \"\"\" Update weights\n",
    "\n",
    "        Args:\n",
    "            w: weight tensor\n",
    "            alpha: learning rate \n",
    "            dw: weight gradient\n",
    "            dr: regularization gradient\n",
    "            lambdaa: regularization lambda parameter\n",
    "\n",
    "        Returns: \n",
    "            updated weight tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self.r is None:\n",
    "            self.r = torch.zeros(w.shape)                \n",
    "            self.v = torch.zeros(w.shape)\n",
    "\n",
    "        r = (self.rho * self.r) + ((1 - self.rho) * (dw**2))\n",
    "        v = (self.epsilon * self.v) - (alpha * ((dw + (lambdaa * dr)) / (torch.sqrt(self.delta + self.r))))\n",
    "\n",
    "        return w + v\n",
    "\n",
    "\n",
    "class AdamOptimizer(ABC):\n",
    "    \"\"\" Adam optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rho1: float=0.9, rho2: float=0.999):\n",
    "        \"\"\" Initialize optimizer\n",
    "        \"\"\"\n",
    "\n",
    "        self.delta = 1e-5\n",
    "        self.rho1 = rho1\n",
    "        self.rho2 = rho2\n",
    "        self.s = None\n",
    "        self.r = None\n",
    "\n",
    "\n",
    "    def __copy__(self):\n",
    "        \"\"\" Return copy of AdamOptimizer\n",
    "        \"\"\"\n",
    "\n",
    "        return AdamOptimizer(self.rho1, self.rho2)\n",
    "\n",
    "\n",
    "    def update(self, w: torch.Tensor, alpha: float, dw: torch.Tensor, dr: torch.Tensor, lambdaa: float=1.0) -> torch.Tensor:\n",
    "        \"\"\" Update weights\n",
    "\n",
    "        Args:\n",
    "            w: weight tensor\n",
    "            alpha: learning rate \n",
    "            dw: weight gradient\n",
    "            dr: regularization gradient\n",
    "            lambdaa: regularization lambda parameter\n",
    "\n",
    "        Returns: \n",
    "            updated weight tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self.s is None:\n",
    "            self.s = torch.zeros(w.shape)                \n",
    "            self.r = torch.zeros(w.shape)                \n",
    "\n",
    "        self.s = (self.rho1 * self.s) + ((1 - self.rho1) * (dw + (lambdaa * dr)))\n",
    "        self.r = (self.rho2 * self.r) + ((1 - self.rho2) * (dw + (lambdaa * dr))**2)\n",
    "        shat = self.s / (1 - self.rho1)\n",
    "        rhat = self.r / (1 - self.rho2)\n",
    "\n",
    "        return w - (alpha * (shat / (torch.sqrt(rhat) + self.delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-association",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-fever",
   "metadata": {},
   "source": [
    "- Goodfellow, Ian, et al. *Deep Learning*. MIT Press, 2017."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
