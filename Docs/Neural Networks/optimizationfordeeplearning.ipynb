{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spiritual-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-tissue",
   "metadata": {},
   "source": [
    "---\n",
    "slug: \"/blog/optimizationfordeeplearning\"\n",
    "date: \"2021-04-12\"\n",
    "title: \"Optimization for Deep Learning\"\n",
    "category: \"2 Deep Learning\"\n",
    "order: 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-thunder",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-filter",
   "metadata": {},
   "source": [
    "Optimization is the process used to minimize the loss function during the training process of a neural network.\n",
    "There are a variety of different approaches to optimization.\n",
    "This post will discuss some of those approaches, including gradient descent, stochastic gradient descent, RMSProp, and Adam.\n",
    "This post will also cover generalized optimization practices, such as momentum as well as adaptive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-metro",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-checkout",
   "metadata": {},
   "source": [
    "Vanilla gradient descent is one of the simplest approaches to optimization.\n",
    "The general process is to reduce the loss function by moving the weights in the opposite direction of the gradient.\n",
    "The weight update performed in gradient descent is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-recipient",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    w &= w - \\alpha \\frac{\\partial L}{\\partial w}&\n",
    "    \\text{Weight update}\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter} &\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-repeat",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-brazilian",
   "metadata": {},
   "source": [
    "Stochastic gradient descent is a variant of gradient descent and is one of the most popular optimization techniques in machine learning.\n",
    "The basic difference between stochastic gradient descent and plain gradient descent is that in stochastic gradient descent, weights are updated after a single randomly-drawn point of data is seen (or a randomly-drawn batch of data points), rather than only when the entire dataset has been seen, as in plain gradient descent.\n",
    "Minibatch stochastic gradient descent is a variant of stochastic gradient descent where the weights of a network are updated using a batch of randomly-drawn input data rather than a single data point.\n",
    "The weight updates for stochastic gradient descent and minibatch stochastic gradient descent are shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-chapter",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    w &= w - \\alpha \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "    &\\text{Weight update}\\\\\n",
    "    w &= w - \\alpha \\left[\\frac{1}{B}\\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i;w))}{\\partial w}\\right]\n",
    "    &\\text{Minibatch weight update}\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter} &\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-albania",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-apple",
   "metadata": {},
   "source": [
    "Momentum is a genralizable method to accelerate optimization that uses a moving average of past gradients to update weights rather than simply the last calculated gradient.\n",
    "The exponentially decaying moving average of gradients, $v$, is the velocity at which the weights of the model move.\n",
    "Nesterov momentum is a momentum method variant that evaluates the gradient after applying the current velocity, rather than before, as in standard momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-sapphire",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    v &= \\epsilon v - \\alpha \\left[\\frac{1}{B}\\sum^B_{i=1}\\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\\right]\n",
    "    & \\text{SGD with Momentum}\\\\\n",
    "    v &= \\epsilon v - \\alpha \\left[\\frac{1}{B}\\sum^B_{i=1}\\frac{\\partial L(y_i, f(x_i; w + \\epsilon v))}{\\partial w}\\right]\n",
    "    & \\text{SGD with Nesterov Momentum}\\\\\n",
    "    w &= w + v \n",
    "    &\\text{Weight update}\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter}&\\\\\n",
    "    \\epsilon &\\rightarrow \\text{model hyperparameter}&\\\\\n",
    "    v & \\rightarrow \\text{velocity}&\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-yacht",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-vertical",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    r &= r + \\left[\\frac{1}{B}\\sum^B_{i=1}\\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\\right]^2\\\\\n",
    "    w &= w - \\frac{\\alpha}{\\delta + \\sqrt{r}} \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "     \\right]\\\\\n",
    "     \\alpha &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "     \\delta &\\rightarrow \\text{small constant, usually }10^{-6}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-thanksgiving",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-ethiopia",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    r &= \\rho r + (1-\\rho) \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "     \\right]^2\\\\\n",
    "     w &= w - \\frac{\\alpha}{\\sqrt{\\delta + r}} \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w))}{\\partial w}\n",
    "     \\right]\\\\\n",
    "     \\alpha &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "     \\rho &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "     \\delta &\\rightarrow \\text{small constant, usually }10^{-6}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-garlic",
   "metadata": {},
   "source": [
    "### RMSProp with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-administration",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    r &= \\rho r + (1-\\rho) \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w + \\epsilon v))}{\\partial w}\n",
    "     \\right]^2\\\\\n",
    "    v &= \\epsilon v - \\frac{\\alpha}{\\sqrt{r}} \\left[\n",
    "         \\frac{1}{B} \\sum^B_{i=1} \\frac{\\partial L(y_i, f(x_i; w + \\epsilon v))}{\\partial w}\n",
    "     \\right]\\\\\n",
    "    w &= w + v\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\rho &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\delta &\\rightarrow \\text{small constant, usually }10^{-6}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-profession",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-tobacco",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    s &= \\rho_1 s + (1 - \\rho_1) \\left[ \n",
    "        \\frac{1}{B} \\sum^B_{i=1} \n",
    "        \\frac{\\partial L(y_i, f(x_i; w + \\epsilon v))}{\\partial w} \n",
    "    \\right]\\\\\n",
    "    r &= \\rho_2 r + (1 - \\rho_2) \\left[ \n",
    "        \\frac{1}{B} \\sum^B_{i=1} \n",
    "        \\frac{\\partial L(y_i, f(x_i; w + \\epsilon v))}{\\partial w} \n",
    "    \\right]^2\\\\\n",
    "    \\hat{s} &= \\frac{s}{1-\\rho_1}\\\\\n",
    "    \\hat{r} &= \\frac{r}{1-\\rho_2}\\\\\n",
    "    w &= w - \\alpha \\left[ \n",
    "        \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}\n",
    "    \\right]\\\\\n",
    "    \\alpha &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\rho_1 &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\rho_2 &\\rightarrow \\text{model hyperparameter}\\\\\n",
    "    \\delta &\\rightarrow \\text{small constant, usually }10^{-6}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-association",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-fever",
   "metadata": {},
   "source": [
    "- Goodfellow, Ian, et al. *Deep Learning*. MIT Press, 2017."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
