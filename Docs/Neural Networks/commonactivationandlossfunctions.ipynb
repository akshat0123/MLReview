{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desirable-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-stephen",
   "metadata": {},
   "source": [
    "---\n",
    "slug: \"/blog/commonactivationandlossfunctions\"\n",
    "date: \"2021-04-07\"\n",
    "title: \"Common Activation and Loss Functions\"\n",
    "category: \"Deep Learning\"\n",
    "order: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-frequency",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-bailey",
   "metadata": {},
   "source": [
    "In my previous post on deep learning, I discussed how to create feed-forward neural network layers, and how create models of arbitrary size with those layers.\n",
    "I also mention that a variety of activation functions can be used in the hidden layers of a neural network, and various loss functions can be used depending on the desired goals for a model.\n",
    "In this post, I will go over some common activation and loss functions and derive their local gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-secondary",
   "metadata": {},
   "source": [
    "### Common Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-suspension",
   "metadata": {},
   "source": [
    "#### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-armstrong",
   "metadata": {},
   "source": [
    "The sigmoid function takes in real-valued input $x$ and returns a real-valued output in the range $[0, 1]$.\n",
    "This activation function is most often used in output layers for binary classification models, although it could technically be used in hidden layers as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-shakespeare",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    \\sigma(x) &= \\frac{1}{1 + e^{-x}} \\\\\n",
    "    \\frac{\\partial \\sigma(x)}{\\partial x} &= \\frac{\\partial}{\\partial x} (1+e^{-x})^{-1}\\\\\n",
    "    &= -(1+e^{-x})^{-2} \\frac{\\partial}{\\partial x} (1 + e^{-x})\\\\\n",
    "    &= -(1+e^{-x})^{-2} \\frac{\\partial}{\\partial x} e^{-x}\\\\\n",
    "    &= -(1+e^{-x})^{-2} e^{-x} \\frac{\\partial}{\\partial x} -x\\\\\n",
    "    &= (1+e^{-x})^{-2} e^{-x}\\\\\n",
    "    &= \\frac{e^{-x}}{(1+e^{-x})^{2}}\\\\\n",
    "    &= \\frac{1}{1+e^{-x}} \\frac{e^{-x}}{1+e^{-x}}\\\\\n",
    "    &= \\frac{1}{1+e^{-x}} \\frac{1 + e^{-x} - 1}{1+e^{-x}}\\\\\n",
    "    &= \\frac{1}{1+e^{-x}} \\left[\\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\right]\\\\\n",
    "    &= \\frac{1}{1+e^{-x}} \\left[1 - \\frac{1}{1+e^{-x}}\\right]\\\\\n",
    "    &= \\sigma(x) [1 - \\sigma(x)]\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-fiction",
   "metadata": {},
   "source": [
    "#### Tanh Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-renewal",
   "metadata": {},
   "source": [
    "The tanh function takes in real-valued input $x$ and returns a real-valued output in the range $[-1, 1]$.\n",
    "This activation function is most often seen as an activation function for hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-dairy",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tanh(x) &= \\frac{\\sinh(x)}{\\cosh(x)}\\\\\n",
    "    &= \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\\\\n",
    "    \\frac{\\partial \\tanh(x)}{\\partial x} &= \n",
    "    \\frac{\n",
    "        \\left[\\frac{\\partial}{\\partial x} (e^x - e^{-x})\\right]\n",
    "        (e^x + e^{-x})\n",
    "        - \n",
    "        (e^x - e^{-x})\n",
    "        \\left[\\frac{\\partial}{\\partial x} (e^x + e^{-x})\\right]\n",
    "    }{\n",
    "        (e^x + e^{-x})^2\n",
    "    }\\\\\n",
    "    &= \n",
    "    \\frac{\n",
    "        (e^x + e^{-x})^2 - (e^x - e^{-x})^2\n",
    "    }{\n",
    "        (e^x + e^{-x})^2\n",
    "    }\\\\\n",
    "    &= 1 - \n",
    "    \\frac{\n",
    "        (e^x - e^{-x})^2\n",
    "    }{\n",
    "        (e^x + e^{-x})^2\n",
    "    }\\\\\n",
    "    &= 1 - \\tanh^2(x)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-marina",
   "metadata": {},
   "source": [
    "#### Rectified Linear Activation Function (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-dance",
   "metadata": {},
   "source": [
    "The ReLU function takes in real-valued input $x$ and returns real-valued output in the range $[0, \\infty)$. \n",
    "The ReLU function is most often seen as an activation function for hidden layers.\n",
    "It should be noted that formally, the derivative for the ReLU function at 0 is undefined. \n",
    "However, in practice, it is often set to 0 when $x$ is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-galaxy",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    r(x) &= \n",
    "    \\begin{cases}\n",
    "        x & x > 0\\\\\n",
    "        0 & x < 0\n",
    "    \\end{cases}\\\\\n",
    "    \\frac{\\partial r(x)}{\\partial x} &= \n",
    "    \\begin{cases}\n",
    "        1 & x > 0\\\\\n",
    "        0 & x < 0\\\\\n",
    "    \\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-burner",
   "metadata": {},
   "source": [
    "#### Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-world",
   "metadata": {},
   "source": [
    "The softmax function takes in a vector of real-values and returns a vector of real values, each in the range $[0, 1]$.\n",
    "The vector $\\vec{s(x)}$ obtained from running vector $x$ through the softmax function always sums to 1.\n",
    "Due to this, the softmax function is most often used as the activation function for output layers in multinomial classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-sociology",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    s(x_i) &= \\frac{e^{x_i}}{\\sum^K_{k=1} e^{x_k}} \\\\\n",
    "    \\frac{\\partial s(x_j)}{\\partial x_i} &= \\frac{\\partial}{\\partial x_i} \\frac{e^{x_j}}{\\sum^K_{k=1} e^{x_k}}\\\\\n",
    "    &= \\frac{\n",
    "        \\left(\\frac{\\partial}{\\partial x_i} e^{x_j}\\right)\\sum^K_{k=1}e^{x_k} - \n",
    "        e^{x_j}\\left(\\frac{\\partial}{\\partial x_i}\\sum^K_{k=1}e^{x_k}\\right) \n",
    "    }{\n",
    "        (\\sum^K_{k=1}e^{x_k})^2\n",
    "    }\\\\\n",
    "    &= \n",
    "    \\begin{cases}\n",
    "        \\frac{\n",
    "            \\left(\\frac{\\partial}{\\partial x_i} e^{x_j}\\right)\\sum^K_{k=1}e^{x_k} - \n",
    "            e^{x_j}\\left(\\frac{\\partial}{\\partial x_i}\\sum^K_{k=1}e^{x_k}\\right) \n",
    "        }{\n",
    "            (\\sum^K_{k=1}e^{x_k})^2\n",
    "        } & i=j\\\\\n",
    "        \\frac{\n",
    "            \\left(\\frac{\\partial}{\\partial x_i} e^{x_j}\\right)\\sum^K_{k=1}e^{x_k} - \n",
    "            e^{x_j}\\left(\\frac{\\partial}{\\partial x_i}\\sum^K_{k=1}e^{x_k}\\right) \n",
    "        }{\n",
    "            (\\sum^K_{k=1}e^{x_k})^2\n",
    "        } & i\\neq j\\\\\n",
    "    \\end{cases}\\\\\n",
    "    &= \n",
    "    \\begin{cases}\n",
    "        \\frac{\n",
    "            e^{x_i}\\sum^K_{k=1}e^{x_k} - \n",
    "            e^{x_i}e^{x_i} \n",
    "        }{\n",
    "            (\\sum^K_{k=1}e^{x_k})^2\n",
    "        } & i=j\\\\\n",
    "        \\frac{\n",
    "            0\\sum^K_{k=1}e^{x_k} - \n",
    "            e^{x_j}e^{x_i}\n",
    "        }{\n",
    "            (\\sum^K_{k=1}e^{x_k})^2\n",
    "        } & i\\neq j\\\\\n",
    "    \\end{cases}\\\\\n",
    "    &= \n",
    "    \\begin{cases}\n",
    "        \\frac{\n",
    "            e^{x_i}(\\sum^K_{k=1}e^{x_k} -  e^{x_i}) \n",
    "        }{\n",
    "            (\\sum^K_{k=1}e^{x_k})^2\n",
    "        } & i=j\\\\\n",
    "        \\frac{\n",
    "            e^{x_i}(-e^{x_j})\n",
    "        }{\n",
    "            (\\sum^K_{k=1}e^{x_k})^2\n",
    "        } & i\\neq j\\\\\n",
    "    \\end{cases}\\\\\n",
    "    &= \n",
    "    \\begin{cases}\n",
    "        s(x_i)(1 - s(x_j))\n",
    "        & i=j\\\\\n",
    "        s(x_i)(0 - s(x_j))\n",
    "        & i\\neq j\\\\\n",
    "    \\end{cases}\\\\\n",
    "    &= s(x_i)(\\delta_{i=j} - s(x_j))\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-share",
   "metadata": {},
   "source": [
    "### Common Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-address",
   "metadata": {},
   "source": [
    "#### Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-belief",
   "metadata": {},
   "source": [
    "The binary cross-entropy loss function is used for models where the desired output is a binary probability.\n",
    "This is necessary in binary classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-convention",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    L_{\\text{BCE}}(y, \\hat{y}) &= -\\left[y\\log \\hat{y} + (1-y)\\log(1-\\hat{y})\\right]\\\\\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} &= \n",
    "    \\frac{\\partial}{\\partial \\hat{y}} -\\left[y\\log \\hat{y} + (1-y)\\log(1-\\hat{y})\\right]\\\\\n",
    "    &= -\\left[\n",
    "        \\frac{\\partial}{\\partial \\hat{y}} y\\log \\hat{y} + \n",
    "        \\frac{\\partial}{\\partial \\hat{y}}(1-y)\\log(1-\\hat{y}) \n",
    "    \\right]\\\\\n",
    "    &= -\\left[  \\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}}  \\right]\\\\\n",
    "    &= \\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat{y}}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-training",
   "metadata": {},
   "source": [
    "#### Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-logic",
   "metadata": {},
   "source": [
    "The cross-entropy function is used for models where the desired output is a probability distribution over $K$ possible classes.\n",
    "This is necessary in multinomial classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-bracket",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    L_{CE}(y, \\hat{y}) &= - \\left[ \\sum^{K}_{k=1} y_k \\log(\\hat{y}_k) \\right] \\\\\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}_i} &= \n",
    "    \\frac{\\partial}{\\partial \\hat{y}_i} - \\left[ \\sum^{K}_{k=1} y_k \\log(\\hat{y}_k) \\right]\\\\\n",
    "    &= -\\left[ \\frac{\\partial}{\\partial \\hat{y_i}} y_i \\log(\\hat{y}_i)  \\right]\\\\\n",
    "    &= -\\frac{y_i}{\\hat{y}_i}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-reception",
   "metadata": {},
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-better",
   "metadata": {},
   "source": [
    "The mean squared error loss function is used for models where the desired output is a real number.\n",
    "This is necessary for regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-country",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned} \n",
    "    L_{\\text{MSE}}(y,\\hat{y}) &= (y - \\hat{y})^2\\\\\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} &= \\frac{\\partial}{\\partial \\hat{y}} (y - \\hat{y})^2\\\\\n",
    "    &= 2(y-\\hat{y})\\frac{\\partial}{\\partial \\hat{y}} (y - \\hat{y})\\\\\n",
    "    &= 2(\\hat{y}-y)\\\\\n",
    "    &\\propto \\hat{y}-y\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
