{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adjusted-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-pioneer",
   "metadata": {},
   "source": [
    "---\n",
    "slug: \"/blog/initializationregularizationdropout\"\n",
    "date: \"2021-04-08\"\n",
    "title: \"Initialization, Regularization, and Dropout\"\n",
    "category: \"Deep Learning\"\n",
    "order: 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-uganda",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-letter",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-halloween",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-tyler",
   "metadata": {},
   "source": [
    "Random initialization just means setting the weights of a hidden layer to small random values before beginning training. \n",
    "A common setting is to simply use random weight values in the range $[-0.1, 0.1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomInitializer(inputdim: int, units: int) -> torch.Tensor:\n",
    "    \"\"\" Returns randomly initialized weights in range [-1, 1]\n",
    "\n",
    "    Args:\n",
    "        inputdim: number of input units         \n",
    "        units: number of units in layer\n",
    "\n",
    "    Returns:\n",
    "        initialized weight tensor            \n",
    "    \"\"\"\n",
    "\n",
    "    return ((torch.rand((inputdim, units)) * 2) - 1) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-subscription",
   "metadata": {},
   "source": [
    "#### Glorot/Xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-array",
   "metadata": {},
   "source": [
    "This method of initialization was designed to prevent possible saturation of parameters during training when using sigmoid and tanh activation functions.\n",
    "Saturation occurs when hidden units become prematurely trapped at a particular value, causing learning to slow or not occur at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-henry",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    W_{ij} &\\sim U\\left[ -\\sqrt{\\frac{1}{n}}, \\sqrt{\\frac{1}{n}} \\right]\\\\\n",
    "    U\\left[-a, a\\right] &= \\text{ uniform distribution in the interval } (-a, a)\\\\\n",
    "    n &= \\text{ the number of units in the previous layer}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GlorotInitializer(inputdim: int, units: int) -> torch.Tensor:\n",
    "    \"\"\" Returns weights initialized using Glorot initialization \n",
    "\n",
    "    Args:\n",
    "        inputdim: number of input units         \n",
    "        units: number of units in layer\n",
    "\n",
    "    Returns:\n",
    "        initialized weight tensor            \n",
    "    \"\"\"\n",
    "\n",
    "    tail = torch.sqrt(torch.Tensor([1/inputdim]))\n",
    "    weights = (torch.rand((inputdim, units)) * tail * 2) - tail\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-reconstruction",
   "metadata": {},
   "source": [
    "#### He"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-museum",
   "metadata": {},
   "source": [
    "This method of initialization was also designed to prevent the possible saturation of parameters during training, but with a focus on the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-discussion",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    W_{ij} &\\sim U\\left[ -\\sqrt{\\frac{2}{n}}, \\sqrt{\\frac{2}{n}}\\right]\\\\\n",
    "    U\\left[-a, a\\right] &= \\text{ uniform distribution in the interval } (-a, a)\\\\\n",
    "    n &= \\text{ the number of units in the previous layer}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeInitializer(inputdim: int, units: int) -> torch.Tensor:\n",
    "    \"\"\" Returns weights initialized using He initialization \n",
    "\n",
    "    Args:\n",
    "        inputdim: number of input units         \n",
    "        units: number of units in layer\n",
    "\n",
    "    Returns:\n",
    "        initialized weight tensor            \n",
    "    \"\"\"\n",
    "\n",
    "    tail = torch.sqrt(torch.Tensor([2/inputdim]))\n",
    "    weights = (torch.rand((inputdim, units)) * tail * 2) - tail\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-arthritis",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-secondary",
   "metadata": {},
   "source": [
    "Regularization is a technique used to reduce model variance.\n",
    "Regularization is not unique to neural networks and has been used with many different models prior to the widespread use of neural networks.\n",
    "In general, the regularization term is a penalty applied to model parameters that favors simpler models to overly complex ones.\n",
    "\n",
    "During training, the penalty $\\Omega(w)$ is added to the loss function, and incorporated into the weight update during gradient descent.\n",
    "The cost function $C(y, \\hat{y}, w)$ below, shows how the penalty would be incorporated into model training.\n",
    "The $\\lambda$ constant multiplied to the penalty term becomes a new hyperparameter for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-rocket",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    C(y, \\hat{y}, w) &= L(y, \\hat{y}) + \\lambda \\Omega(w)\\\\\n",
    "    w &= w - \\alpha \\frac{\\partial C}{\\partial w} \\\\\n",
    "    &= w - \\alpha \\left[ \\frac{\\partial L}{\\partial w} + \\lambda\\frac{\\partial \\Omega(w)}{\\partial w} \\right]\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-average",
   "metadata": {},
   "source": [
    "#### L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-punch",
   "metadata": {},
   "source": [
    "L1 regularization, also known as lasso regression, shrinks parameter values by adding the sum of the absolute value of weights to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-integrity",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Omega(w) &= \\sum^{N}_{i=1} |w_i| & \\text{L1 Regularization}\\\\\n",
    "    \\frac{\\partial \\Omega(w)}{\\partial w_i} &= \n",
    "    \\frac{\\partial}{\\partial w_i}\\sum^{N}_{j=1} |w_j| & \\text{Derivative}\\\\\n",
    "    &= \\frac{\\partial}{\\partial w_i} |w_i| &\\\\\n",
    "    &= \\frac{w_i}{|w_i|} &\\\\\n",
    "    &= \\begin{cases}\n",
    "        1 & w_i > 0\\\\\n",
    "        -1 & w_i < 0\\\\\n",
    "    \\end{cases}&\\\\\n",
    "    w_i &= \\begin{cases}\n",
    "        w_i - \\alpha \\left[ \\frac{\\partial L}{\\partial w_i} + \\lambda \\right] & w_i > 0\\\\\n",
    "        w_i - \\alpha \\left[ \\frac{\\partial L}{\\partial w_i} - \\lambda \\right] & w_i < 0\\\\\n",
    "    \\end{cases}& \\text{Weight update}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1Regularizer(w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    penalty = torch.sum(torch.abs(torch.clone(w)))\n",
    "    grad = torch.clone(w)\n",
    "    grad[grad >= 0] = 1\n",
    "    grad[grad < 0] = -1\n",
    "\n",
    "    return penalty, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-slovakia",
   "metadata": {},
   "source": [
    "#### L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-tuesday",
   "metadata": {},
   "source": [
    "L2 regularization, also known as ridge regression, shrinks parameter values by adding the sum of the squares of weights to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-legend",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Omega(w) &= \\sum^N_{i=1} w_i^2 &\\text{L2 Regularization}\\\\\n",
    "    \\frac{\\partial \\Omega(w)}{\\partial w_i} &=\n",
    "    \\frac{\\partial}{\\partial w_i}\\sum^N_{j=1} w_j^2 &\\text{Derivative}\\\\\n",
    "    &= \\frac{\\partial}{\\partial w_i} w_i^2&\\\\\n",
    "    &= 2w_i&\\\\\n",
    "    w_i &= w_i - \\alpha\\left[\\frac{\\partial L}{\\partial w_i} + 2\\lambda w_i  \\right] &\\text{Weight update}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2Regularizer(w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    penalty = torch.sum(torch.square(torch.clone(w)))\n",
    "    grad = 2 * torch.clone(w)\n",
    "\n",
    "    return penalty, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-exhaust",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-notice",
   "metadata": {},
   "source": [
    "Dropout is a simple regularization approach designed specifically for neural networks.\n",
    "The dropout technique involves ignoring some of the hidden units in a layer during the training of a neural network.\n",
    "When only a random proportion of network units are used during training, this approach is similar to the bagging ensemble method, as it is comparable to using multiple different network architectures simultaneously during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-remark",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-latest",
   "metadata": {},
   "source": [
    "- Glorot, X., & Bengio, Y. (2010). *Understanding the difficulty of training deep feedforward neural networks*. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 249–256). PMLR.\n",
    "- He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) (pp. 1026–1034). IEEE Computer Society.\n",
    "- Goodfellow, Ian, et al. *Deep Learning*. MIT Press, 2017. \n",
    "- Hastie, Trevor, et al. *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
