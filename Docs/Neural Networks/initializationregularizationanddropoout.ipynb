{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adjusted-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-pioneer",
   "metadata": {},
   "source": [
    "---\n",
    "slug: \"/blog/initializationregularizationdropout\"\n",
    "date: \"2021-04-08\"\n",
    "title: \"Initialization, Regularization, and Dropout\"\n",
    "category: \"Deep Learning\"\n",
    "order: 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-uganda",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-letter",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-halloween",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-tyler",
   "metadata": {},
   "source": [
    "Random initialization just means setting the weights of a hidden layer to small random values before beginning training. \n",
    "A common setting is to simply use random weight values in the range $[-0.1, 0.1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-subscription",
   "metadata": {},
   "source": [
    "#### Glorot/Xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-array",
   "metadata": {},
   "source": [
    "This method of initialization was designed to prevent possible saturation of parameters during training when using sigmoid and tanh activation functions.\n",
    "Saturation occurs when hidden units become prematurely trapped at a particular value, causing learning to slow or not occur at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-henry",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    W_{ij} &\\sim U\\left[ -\\sqrt{\\frac{1}{n}}, \\sqrt{\\frac{1}{n}} \\right]\\\\\n",
    "    U\\left[-a, a\\right] &= \\text{ uniform distribution in the interval } (-a, a)\\\\\n",
    "    n &= \\text{ the number of units in the previous layer}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-reconstruction",
   "metadata": {},
   "source": [
    "#### He"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-museum",
   "metadata": {},
   "source": [
    "This method of initialization was also designed to prevent the possible saturation of parameters during training, but with a focus on the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-discussion",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    W_{ij} &\\sim U\\left[ -\\sqrt{\\frac{2}{n}}, \\sqrt{\\frac{2}{n}}\\right]\\\\\n",
    "    U\\left[-a, a\\right] &= \\text{ uniform distribution in the interval } (-a, a)\\\\\n",
    "    n &= \\text{ the number of units in the previous layer}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-arthritis",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-festival",
   "metadata": {},
   "source": [
    "Regularization is a technique used to reduce model variance.\n",
    "Regularization is not unique to neural networks and has been used with many different models prior to the widespread use of neural networks.\n",
    "In general, the regularization term is a penalty applied to model parameters that favors simpler models to overly complex ones.\n",
    "\n",
    "During training, the penalty $\\Omega(w)$ is added to the loss function, and incorporated into the weight update during gradient descent.\n",
    "The cost function $C(y, \\hat{y}, w)$ below, shows how the penalty would be incorporated into model training.\n",
    "The $\\lambda$ constant multiplied to the penalty term becomes a new hyperparameter for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-respondent",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    C(y, \\hat{y}, w) &= L(y, \\hat{y}) + \\lambda \\Omega(w)\\\\\n",
    "    w &= w - \\alpha \\frac{\\partial C}{\\partial w} \\\\\n",
    "    &= w - \\alpha \\left[ \\frac{\\partial L}{\\partial w} + \\lambda\\frac{\\partial \\Omega(w)}{\\partial w} \\right]\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-qualification",
   "metadata": {},
   "source": [
    "#### L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-shore",
   "metadata": {},
   "source": [
    "L1 regularization, also known as lasso regression, shrinks parameter values by adding the sum of the absolute value of weights to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-cambridge",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Omega(w) &= \\sum^{N}_{i=1} |w_i| & \\text{L1 Regularization}\\\\\n",
    "    \\frac{\\partial \\Omega(w)}{\\partial w_i} &= \n",
    "    \\frac{\\partial}{\\partial w_i}\\sum^{N}_{j=1} |w_j| & \\text{Derivative}\\\\\n",
    "    &= \\frac{\\partial}{\\partial w_i} |w_i| &\\\\\n",
    "    &= \\frac{w_i}{|w_i|} &\\\\\n",
    "    &= \\begin{cases}\n",
    "        1 & w_i > 0\\\\\n",
    "        -1 & w_i < 0\\\\\n",
    "    \\end{cases}&\\\\\n",
    "    w_i &= \\begin{cases}\n",
    "        w_i - \\alpha \\left[ \\frac{\\partial L}{\\partial w_i} + \\lambda \\right] & w_i > 0\\\\\n",
    "        w_i - \\alpha \\left[ \\frac{\\partial L}{\\partial w_i} - \\lambda \\right] & w_i < 0\\\\\n",
    "    \\end{cases}& \\text{Weight update}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-freeze",
   "metadata": {},
   "source": [
    "#### L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-static",
   "metadata": {},
   "source": [
    "L2 regularization, also known as ridge regression, shrinks parameter values by adding the sum of the squares of weights to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-asian",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Omega(w) &= \\sum^N_{i=1} w_i^2 &\\text{L2 Regularization}\\\\\n",
    "    \\frac{\\partial \\Omega(w)}{\\partial w_i} &=\n",
    "    \\frac{\\partial}{\\partial w_i}\\sum^N_{j=1} w_j^2 &\\text{Derivative}\\\\\n",
    "    &= \\frac{\\partial}{\\partial w_i} w_i^2&\\\\\n",
    "    &= 2w_i&\\\\\n",
    "    w_i &= w_i - \\alpha\\left[\\frac{\\partial L}{\\partial w_i} + 2\\lambda w_i  \\right] &\\text{Weight update}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-exhaust",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-remark",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-latest",
   "metadata": {},
   "source": [
    "- Glorot, X., & Bengio, Y. (2010). *Understanding the difficulty of training deep feedforward neural networks*. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 249–256). PMLR.\n",
    "- He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) (pp. 1026–1034). IEEE Computer Society.\n",
    "- Goodfellow, Ian, et al. *Deep Learning*. MIT Press, 2017. \n",
    "- Hastie, Trevor, et al. *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
