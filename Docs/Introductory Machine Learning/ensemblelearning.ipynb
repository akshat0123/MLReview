{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "structural-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-design",
   "metadata": {},
   "source": [
    "---\n",
    "slug: \"/blog/ensemblelearning\"\n",
    "date: \"2021-03-15\"\n",
    "title: \"Ensemble Learning\"\n",
    "category: \"1 Machine Learning\"\n",
    "order: 6\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-convergence",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Ensemble learning is the process of using multiple models to improve overall performance in a machine learning tasks.\n",
    "This post briefly touches on two popular ensemble methods for decision tree learners: Random Forests and Gradient Boosted Trees.\n",
    "Random forests are a bagging approach, in which models are trained independently of each other and combined to produce output.\n",
    "Gradient Boosted Trees are a boosting approach, in which models are trained iteratively, each successive model being trained with regard to the errors of the last.\n",
    "This post will discuss only the classification version of each - although both approaches can be used for regression as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-palmer",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "The basic approach to creating random forests is shown in the pseudo-code below.\n",
    "A bootstrap sample is drawn from data set $X$ of size $N$ with dimension $D$.\n",
    "A decision tree is built on this bootstrap and added to the ensemble. \n",
    "This is repeated $T$ times, $T$ being a model parameter indicating the number of trees to include in the ensemble.\n",
    "This post will also consider the maximum depth of each tree and the minimum size of a leaf as parameters of the Random Forest model.\n",
    "\n",
    "``` \n",
    "for t = 1 to T:\n",
    "   Retrieve a bootstrap sample\n",
    "   Grow a full decision tree on the sample with random features\n",
    "   Add it to the ensemble\n",
    "       \n",
    "return ensemble           \n",
    "```\n",
    "\n",
    "Given a trained random forest, outputs are produced by taking the majority output class of all decision trees in the ensemble.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    m_t &= \\text{model $t$ of the ensemble} &\\\\\n",
    "    f(x) &= \\text{majority vote}\\{m_t(x)\\}^T_{t=1} & \\text{[Output]}\\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-mixture",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Code for a random forest classifier is shown in the block below.\n",
    "It assumes use of the decision tree classifier class described in the earlier post on decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collective-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlr.Models.DecisionTree import DecisionTreeClassifier \n",
    "from typing import List\n",
    "from tqdm import trange\n",
    "import torch\n",
    "\n",
    "\n",
    "class RandomForestClassifier:\n",
    "\n",
    "\n",
    "    def __init__(self, numTrees: int=5, maxDepth: int=None, leafSize: int=1, bootstrapRatio: float=0.3) -> None:\n",
    "        \"\"\" Instantiate random forest classifier\n",
    "\n",
    "        Args:\n",
    "            numTrees: number of trees to build\n",
    "            maxDepth: the maximum allowed depth of each tree\n",
    "            leafSize: the minimum number of data points required to split a node \n",
    "            bootstrapRatio: ratio of training data to use for each bootstrap\n",
    "        \"\"\"\n",
    "        self.forest = []\n",
    "        self.numTrees = numTrees\n",
    "        self.maxDepth = maxDepth\n",
    "        self.leafSize = leafSize\n",
    "        self.bootstrapRatio = bootstrapRatio\n",
    "\n",
    "\n",
    "    def fit(self, x: torch.Tensor, y: torch.Tensor, classes: List[float]) -> None:\n",
    "        \"\"\" Fit random forest model to dataset\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "            y: input labels\n",
    "            classes: list of unique possible labels\n",
    "        \"\"\"\n",
    "\n",
    "        for i in trange(self.numTrees):\n",
    "            \n",
    "            # Create bootstrap sample\n",
    "            rargs = torch.randperm(x.shape[0]) \n",
    "            x, y = x[rargs], y[rargs] \n",
    "            bidx = int(x.shape[0] * self.bootstrapRatio)\n",
    "\n",
    "            tree = DecisionTreeClassifier(maxDepth=self.maxDepth, leafSize=self.leafSize)\n",
    "            tree.fit(x[:bidx], y[:bidx], classes)\n",
    "            self.forest.append(tree)\n",
    "\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Predict outcomes given input\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "\n",
    "        Returns:\n",
    "            tensor of class labels            \n",
    "        \"\"\"\n",
    "\n",
    "        return torch.mode(\n",
    "            torch.cat(\n",
    "                [tree.predict(x) for tree in self.forest], dim=1\n",
    "            ), dim=1\n",
    "        ).values[:, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-mayor",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees\n",
    "\n",
    "The basic approach to creating gradient boosted trees is shown in the pseudo-code below.\n",
    "A shallow decision tree is trained on data set $X$ of size $N$ with dimension $D$. \n",
    "The following decision tree is trained on the negative gradient on a chosen loss function.\n",
    "In this post, cross entropy will be used as a loss function, and the number of decision trees in the ensemble, the maximum depth of each tree, and the minimum size of a leaf will all be considered parameters of the model.\n",
    "The softmax function is used to account for multiclass classification problems.\n",
    "Decision tree regressors are used internally, even though the ensemble itself is a classifier\n",
    "\n",
    "```\n",
    "fit tree to data\n",
    "calculate gradient\n",
    "\n",
    "while loss not acceptable:\n",
    "    fit new tree to negative gradient\n",
    "    calculate gradient\n",
    "```\n",
    "\n",
    "The following computation graph provides some insight into how gradients are calculated in the model update process. \n",
    "Outputs $o_1 .. o_k$, probabilities for each of $K$ classes are produced by a single internal decision tree regressor.\n",
    "The softmax function is performed on this vector $\\vec{o}$ to produce $\\vec{p}$. \n",
    "The loss is then calculated using $\\vec{p}$ and $\\vec{y}$.\n",
    "The gradient is calculated with respect to model outputs $o_1$ to $o_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continuing-short",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%tikz s=400,400 -sc=2\n",
    "\\node[draw, circle] (o1) at (0, 0) {$o_1$};\n",
    "\\node[draw, circle] (o2) at (0, -1) {$o_2$};\n",
    "\\node[draw, circle] (ok) at (0, -2.5) {$o_k$};\n",
    "\\node[draw, circle] (p1) at (1, 0) {$p_1$};\n",
    "\\node[draw, circle] (p2) at (1, -1) {$p_2$};\n",
    "\\node[draw, circle] (pk) at (1, -2.5) {$p_k$};\n",
    "\\node[draw, circle] (y) at (2, -1.75) {$y$};\n",
    "\\node[draw, circle] (l) at (3, -1.25) {$L$};\n",
    "\\node (sm) at (1, 0.5) {Softmax};\n",
    "\\draw (0.75, 0.25) rectangle (1.25, -2.75);\n",
    "\\draw[dotted, line width=1pt] (o2) -- (ok);\n",
    "\\draw[dotted, line width=1pt] (p2) -- (pk);\n",
    "\\draw[->] (o1) -- (0.75, 0);\n",
    "\\draw[->] (o2) -- (0.75, -1);\n",
    "\\draw[->] (ok) -- (0.75, -2.5);\n",
    "\\draw[->] (y) -- (l);\n",
    "\\draw[->] (1.25, -1.25) -- (l);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-airport",
   "metadata": {},
   "source": [
    "Once trained, the ensemble produces output by adding up the outputs produced by all the internal decision tree regressors, and taking the class with the maximum probability as the output label.\n",
    "Derivations for the model update are shown in the derivations section of this post.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    s(x_i) &= \\frac{e^{x_i}}{\\sum^{K}_{k=1} e^{x_k}} & \\text{[Softmax function]}\\\\\n",
    "    L(y, \\hat{y}) &= - \\left[ \\sum^{K}_{k=1} y_k\\log \\hat{y}_k \\right] & \\text{[Cross entropy loss]}\\\\\n",
    "    f(x) &= \\max_k\\left[\\sum^T_{t=1} f_t(x)\\right] & \\text{[Output]}\\\\\n",
    "    f(x) &= f(x) - \\alpha \\frac{\\partial L}{\\partial f(x)} & \\text{[Model update]}\\\\\n",
    "    &= f(x) - \\alpha \\left[ s(f(x) - y) \\right]&\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-initial",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Code for a gradient boosted trees classifier is shown in the block below.\n",
    "It assumes use of the decision tree regressor class described in the earlier post on decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liked-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlr.Models.DecisionTree import DecisionTreeRegressor \n",
    "from typing import List\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def createOneHotColumn(data: np.ndarray, colidx: int=0) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Turn single categorical column to one-hot columns\n",
    "\n",
    "    Args:\n",
    "        data: input data\n",
    "        colidx: column to transform\n",
    "\n",
    "    Returns:\n",
    "        one-hot vectors of specified column\n",
    "        list of column names for new one-hot vectors \n",
    "    \"\"\"\n",
    "\n",
    "    ucols = np.unique(data[:, colidx])\n",
    "    uidxs = { ucols[i]: i for i in range(len(ucols)) }\n",
    "    urows = np.zeros((data.shape[0], ucols.shape[0]))\n",
    "\n",
    "    for ridx in range(data.shape[0]):\n",
    "        urows[ridx, uidxs[data[ridx, colidx]]] = 1.0\n",
    "\n",
    "    return urows, ucols        \n",
    "\n",
    "\n",
    "def Softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" Apply softmax function to tensor\n",
    "\n",
    "    Args:\n",
    "        x: input tensor\n",
    "\n",
    "    Returns:\n",
    "        tensor with softmax function applied to all members\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim=1)[:, None]\n",
    "\n",
    "\n",
    "class GradientBoostedTreeClassifier:\n",
    "\n",
    "\n",
    "    def __init__(self, maxDepth: int=3, leafSize: int=1):\n",
    "        \"\"\" Initializes GradientBoostedTreeClassifier class\n",
    "\n",
    "        Args:\n",
    "            maxDepth: maximum depth of internal trees\n",
    "            leafSize: minimum size of node for each tree\n",
    "        \"\"\"\n",
    "        self.maxDepth = maxDepth\n",
    "        self.leafSize = leafSize\n",
    "        self.layers = []\n",
    "\n",
    "\n",
    "    def fit(self, x: torch.Tensor, y: torch.Tensor, alpha: float=0.01, numTrees: int=10) -> None:\n",
    "        \"\"\" Fit classifier to input data\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "            y: input labels\n",
    "            alpha: alpha parameter for weight update\n",
    "        \"\"\"\n",
    "\n",
    "        y = torch.Tensor(createOneHotColumn(y.numpy())[0])\n",
    "        self.numClasses = y.shape[1]\n",
    "        ypred = torch.full(y.shape, 1 / y.shape[1])\n",
    "\n",
    "        layers, fx = [], y - ypred\n",
    "        for i in trange(numTrees):\n",
    "\n",
    "            trees = [ DecisionTreeRegressor(maxDepth=self.maxDepth, leafSize=self.leafSize) for i in range(y.shape[1]) ]\n",
    "            for i in range(len(trees)): \n",
    "                trees[i].fit(x, fx[:, i])\n",
    "\n",
    "            layers.append(trees)\n",
    "\n",
    "            probs = self.probs(x, trees)\n",
    "            fx -= alpha * (probs - y)\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "\n",
    "    def probs(self, x: torch.Tensor, trees: List[DecisionTreeRegressor]) -> torch.Tensor:\n",
    "        \"\"\" Determine probability of belonging to each class\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "            trees: list of decision trees, one for each class\n",
    "\n",
    "        Returns:\n",
    "            probability for each input\n",
    "        \"\"\"\n",
    "\n",
    "        probs = [trees[i].predict(x)[:, None] for i in range(len(trees))]\n",
    "        return Softmax(torch.cat(probs, dim=1))\n",
    "\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Predict labels\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "\n",
    "        Returns:\n",
    "            labels for each member of input\n",
    "        \"\"\"\n",
    "\n",
    "        ypred = torch.zeros((x.shape[0], self.numClasses))\n",
    "        for trees in self.layers:\n",
    "            probs = self.probs(x, trees)\n",
    "            ypred += probs\n",
    "\n",
    "        return torch.argmax(ypred, dim=1)[:, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-pharmacy",
   "metadata": {},
   "source": [
    "### Derivations\n",
    "\n",
    "Derivative of softmax output $p_j$ with respect to single $f(x)$ output $o_i$ to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial p_j}{\\partial o_j} &= \\frac{\\partial}{\\partial o_i} \\frac{e^{o_j}}{\\sum^K_{k=1} e^{o_k}} \\\\\n",
    "    &= \\frac{ (\\frac{\\partial}{\\partial o_i} e^{o_j})(\\sum^K_{k=1} e^{o_k}) - (e^{o_j})(\\frac{\\partial}{\\partial o_i} \\sum^K_{k=1} e^{o_k}) }{ (\\sum^K_{k=1} e^{o_k})^2 }\\\\\n",
    "    &= \\begin{cases} i = j & \\frac{ e^{o_j} \\sum^K_{k=1}e^{o_k} - (e^{o_j})^2 }{ (\\sum^K_{k=1}e^{o_k})^2 }\\\\ i \\neq j & \\frac{ 0 - e^{o_j}e^{o_i} }{ (\\sum^K_{k=1}e^{o_k})^2 }\\\\ \\end{cases}\\\\\n",
    "    &= \\begin{cases} i = j & p_j - p_j^2\\\\ i \\neq j & -p_jp_i\\\\ \\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Derivative of loss $L$ with with respect to single $f(x)$ output $o_i$ to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial o_i} &= \\frac{\\partial}{\\partial o_i} \\left[- \\sum^K_{k=1} y_k \\log p_k \\right]\\\\\n",
    "    &= - \\left[\\sum^K_{k=1} \\frac{\\partial}{\\partial o_i} y_k \\log p_k \\right]\\\\\n",
    "    &= - \\left[\\sum^K_{k=1} \\frac{y_k}{p_k} \\frac{\\partial}{\\partial o_i}  p_k \\right]\\\\\n",
    "    &= - \\left[ \\frac{y_i (p_i - p_i^2)}{p_i} + \\sum^K_{k\\neq i} \\frac{-y_k(p_kp_i)}{p_k} \\right]\\\\\n",
    "    &= - \\left[ y_i (1-p_i) - \\sum^K_{k\\neq i} y_k p_i \\right]\\\\\n",
    "    &= - \\left[ y_i - p_iy_i - p_i\\sum^K_{k\\neq i} y_k \\right]\\\\\n",
    "    &= - \\left[ y_i - p_i\\sum^K_{k=1} y_k \\right]\\\\\n",
    "    &= - \\left[ y_i - p_i \\right]\\\\\n",
    "    &=  p_i - y_i\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-revelation",
   "metadata": {},
   "source": [
    "### Resources\n",
    "- Russell, Stuart J., et al. Artificial Intelligence: A Modern Approach. 3rd ed, Prentice Hall, 2010.\n",
    "- Bendersky, Eli. “The Softmax Function and Its Derivative.” Eli Bendersky’s Website. 2016. https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/. Accessed 17 Mar. 2021.\n",
    "- Liu, Ethen. Gradient Boosting Machine (GBM). http://ethen8181.github.io/machine-learning/trees/gbm/gbm.html. Accessed 17 Mar. 2021."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
