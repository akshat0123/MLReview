\section{Ensemble Learning}

\subsection{Random Forests}

\begin{figure}[h]
    \begin{lstlisting}[gobble=8]
        for b = 1 to B:
            Retrieve a bootstrap sample
            Grow a full decision tree on the sample with random features
        return ensemble 
    \end{lstlisting}
    \caption{Random Forest construction process}
\end{figure}

\begin{align*}
    f(x) &= \frac{1}{B}\sum^{B}_{b=1} T_b(x) & \text{[Regression]}\\
    f(x) &= \text{majority vote} \{T_b(x)\}^{B}_{1} & \text{[Classification]}\\
\end{align*}

\subsection{Gradient Boosted Trees}

\begin{figure}[h]
    \begin{lstlisting}[gobble=8]
        fit tree to data
        calculate gradient

        while loss not acceptable:
            fit new tree to negative gradient
    \end{lstlisting}
\end{figure}

\begin{align*}
    s(x_i) &= \frac{e^{x_i}}{\sum^K_{k=1} e^{x_k}} & \text{[Softmax Function]}\\
    L_{\text{CE}} (y, \hat{y}) &= -\sum^K_{k=1} y \log \hat{y}& \text{[Cross Entropy Loss]}\\
    f(x) &= f(x) - \alpha \frac{\partial L(y, f(x))}{\partial f(x)} & \text{[Model update]}\\
    &= f(x) - \alpha \left[s(f(x)) - y\right]&\\
\end{align*}

\input{tikz/gradient_boosted_trees_computation_graph}

\subsubsection{Derivations}
Derivative of softmax output $p_j$ with respect to single $f(x)$ output $o_i$ to:\\
\begin{align*}
    \frac{\partial p_j}{\partial o_j} &= \frac{\partial}{\partial o_i} \frac{e^{o_j}}{\sum^K_{k=1} e^{o_k}} \\
    &= \frac{ (\frac{\partial}{\partial o_i} e^{o_j})(\sum^K_{k=1} e^{o_k}) - (e^{o_j})(\frac{\partial}{\partial o_i} \sum^K_{k=1} e^{o_k}) }{ (\sum^K_{k=1} e^{o_k})^2 }\\
    &= \begin{cases} i = j & \frac{ e^{o_j} \sum^K_{k=1}e^{o_k} - (e^{o_j})^2 }{ (\sum^K_{k=1}e^{o_k})^2 }\\ i \neq j & \frac{ 0 - e^{o_j}e^{o_i} }{ (\sum^K_{k=1}e^{o_k})^2 }\\ \end{cases}\\
    &= \begin{cases} i = j & p_j - p_j^2\\ i \neq j & -p_jp_i\\ \end{cases}\\
\end{align*}

Derivative of loss $L$ with with respect to single $f(x)$ output $o_i$ to:\\
\begin{align*}
    \frac{\partial L}{\partial o_i} &= \frac{\partial}{\partial o_i} \left[- \sum^K_{k=1} y_k \log p_k \right]\\
    &= - \left[\sum^K_{k=1} \frac{\partial}{\partial o_i} y_k \log p_k \right]\\
    &= - \left[\sum^K_{k=1} \frac{y_k}{p_k} \frac{\partial}{\partial o_i}  p_k \right]\\
    &= - \left[ \frac{y_i (p_i - p_i^2)}{p_i} + \sum^K_{k\neq i} \frac{-y_k(p_kp_i)}{p_k} \right]\\
    &= - \left[ y_i (1-p_i) - \sum^K_{k\neq i} y_k p_i \right]\\
    &= - \left[ y_i - p_iy_i - p_i\sum^K_{k\neq i} y_k \right]\\
    &= - \left[ y_i - p_i\sum^K_{k=1} y_k \right]\\
    &= - \left[ y_i - p_i \right]\\
    &=  p_i - y_i\\
\end{align*}

\textbf{Resources}
\begin{enumerate}
    \item Artificial Intelligence: A Modern Approach~\cite{russell1}
    \item The Softmax function and its derivative~\cite{bendersky1}
    \item Gradient Boosting Machine (GBM)~\cite{liu1}
\end{enumerate}
