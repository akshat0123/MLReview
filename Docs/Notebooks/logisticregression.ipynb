{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latter-island",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tikzmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext tikzmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-gabriel",
   "metadata": {},
   "source": [
    "---\n",
    "slug: \"/blog/logisticregression\"\n",
    "date: \"2021-03-15\"\n",
    "title: \"Logistic Regression\"\n",
    "category: \"Machine Learning\"\n",
    "order: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-intellectual",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Logistic regression is a parametric classification method that is used to estimate binary class output given real-valued input vectors. \n",
    "In this post, cross entropy loss is used as the loss function and gradient descent (or batched gradient descent) is used to learn parameters.\n",
    "\n",
    "\n",
    "The computation graph below shows how logistic regression works. \n",
    "The dot product of each input, a vector $\\vec{x}$ of size $D$, and weight vector $\\vec{w}$ transposed is taken, resulting in $z$.\n",
    "$z$ is put through the sigmoid function, and the loss is calculated using the mean squared error on label $y$ and $\\sigma(z)$.\n",
    "\n",
    "The bias term $\\beta$ is ignored for the purposes of this post, but can easily be appended to weight vector $\\vec{w}$ after appending a $1$ to an input vector $\\vec{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brief-ceiling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%tikz -s 300,300 -sc=2\n",
    "\\node[draw, circle] (x) at (0, 1) {$x$};\n",
    "\\node[draw, circle] (z) at (1, 1) {$z$};\n",
    "\\node[draw, circle] (s) at (2, 1) {$\\sigma(z)$};\n",
    "\\node[draw, circle] (y) at (2, 0) {$y$};\n",
    "\\node[draw, circle] (l) at (3, 1) {$L$};\n",
    "\\draw[->] (x) edge node[midway, above] {$w$} (z);\n",
    "\\draw[->] (z) -- (s);\n",
    "\\draw[->] (s) -- (l);\n",
    "\\draw[->] (y) -- (l);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-isolation",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Given a data set $X$ of size $N$ with $D$ dimensions, parameters $w$ must be learned that minimize our loss function $L_{CE}(y, \\hat{y})$.\n",
    "The weight vector is learned using gradient descent.\n",
    "The derivation for term $\\frac{\\partial L}{\\partial w}$ in the weight update is displayed in the derivations section of this post.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\sigma(z) &= \\frac{1}{1 + e^{-z}} & \\text{[Logistic function]}\\\\\n",
    "    L_{\\text{CE}}(y, \\hat{y}) &= -\\left[ y\\log \\hat{y} + (1-y)\\log(1-\\hat{y}) \\right]& \\text{[Cross entropy loss]}\\\\\n",
    "    w_i &= w_i - \\alpha \\frac{\\partial L}{\\partial w_i} & \\text{[Weight update]}\\\\\n",
    "    &= w_i - \\alpha \\left[x_i (\\sigma(z) - y)\\right] &\\\\\n",
    "    w_i &= w_i - \\alpha \\frac{1}{B}\\sum^B_{j=1}x_{j,i}(\\sigma(z)-y) & \\text{[Batch weight update]}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-cisco",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Code for a logistic regression classifier is shown in the block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "female-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import trange\n",
    "import torch\n",
    "\n",
    "\n",
    "def ErrorRate(y: torch.Tensor, yhat: torch.Tensor) -> float:\n",
    "    \"\"\" Calculate error rate (1 - accuracy)\n",
    "\n",
    "    Args:\n",
    "        y: true labels\n",
    "        yhat: predicted labels\n",
    "\n",
    "    Returns:\n",
    "        error rate\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.sum((y != yhat).float()) / y.shape[0]\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\" Instantiate logistic regression classifier\n",
    "        \"\"\"\n",
    "\n",
    "        self.w = None\n",
    "        self.calcError = ErrorRate\n",
    "\n",
    "\n",
    "    def fit(self, x, y, alpha=1e-4, epochs=1000, batch=32) -> None:\n",
    "        \"\"\" Fit logistic regression classifier to dataset\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "            y: input labels\n",
    "            alpha: alpha parameter for weight update\n",
    "            epochs: number of epochs to train\n",
    "            batch: size of batches for training\n",
    "        \"\"\"\n",
    "\n",
    "        self.w = torch.rand((1, x.shape[1]))\n",
    "\n",
    "        epochs = trange(epochs, desc='Error')\n",
    "        for epoch in epochs:\n",
    "\n",
    "            start, end = 0, batch\n",
    "            for b in range((x.shape[0]//batch)+1):\n",
    "                hx = self.probs(x[start:end])\n",
    "                dw = self.calcGradient(x[start:end], y[start:end], hx)\n",
    "                self.w = self.w - (alpha * dw)\n",
    "                start += batch\n",
    "                end += batch\n",
    "\n",
    "            hx = self.predict(x)\n",
    "            error = self.calcError(y, hx)\n",
    "            epochs.set_description('Err: %.4f' % error)\n",
    "\n",
    "\n",
    "    def probs(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Determine probability of label being 1\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "\n",
    "        Returns:\n",
    "            probability for each member of input\n",
    "        \"\"\"\n",
    "\n",
    "        hx = 1 / (1 + torch.exp(-torch.einsum('ij,kj->i', x, self.w)))[:, None]\n",
    "        return hx\n",
    "\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Predict labels\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "\n",
    "        Returns:\n",
    "            labels for each member of input\n",
    "        \"\"\"\n",
    "\n",
    "        hx = self.probs(x)\n",
    "        hx = (hx >= 0.5).float()\n",
    "        return hx\n",
    "\n",
    "\n",
    "    def calcGradient(self, x: torch.Tensor, y: torch.Tensor, hx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Calculate weight gradient\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "            y: input labels\n",
    "            hx: predicted probabilities\n",
    "\n",
    "        Returns:\n",
    "            tensor of gradient values the same size as weights\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.sum(x * (hx - y), dim=0) / x.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-spokesman",
   "metadata": {},
   "source": [
    "### Derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-sheep",
   "metadata": {},
   "source": [
    "Derivative of loss function $L$ with respect to sigmoid output $a=\\sigma(z)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial a} &= \\frac{\\partial}{\\partial a} - \\left[y\\log a + (1-y) \\log (1-a)\\right]\\\\\n",
    "    &= - \\left[\n",
    "        y\\frac{\\partial}{\\partial a}\\log a +\n",
    "        (1-y)\\frac{\\partial}{\\partial a}\\log (1-a)\n",
    "    \\right]\\\\\n",
    "    &= - \\left[\n",
    "        \\frac{y}{a}\\frac{\\partial}{\\partial a}a +\n",
    "        \\frac{(1-y)}{(1-a)}\\frac{\\partial}{\\partial a}(1-a)\n",
    "    \\right]\\\\\n",
    "    &= -\\left[\\frac{y}{a} - \\frac{(1-y)}{(1-a)}\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Derivative of sigmoid function $\\sigma(z)$ with respect to sigmoid input $z$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial \\sigma(z)}{\\partial z} &= \\frac{\\partial}{\\partial z}\\frac{1}{1 + e^{-z}}\\\\\n",
    "    &= \\frac{\\partial}{\\partial z}(1 + e^{-z})^{-1}\\\\\n",
    "    &= -(1 + e^{-z})^{-2}\\frac{\\partial}{\\partial z}(1 + e^{-z})\\\\\n",
    "    &= -(1 + e^{-z})^{-2}\\frac{\\partial}{\\partial z}(e^{-z})\\\\\n",
    "    &= \\frac{-e^{-z}}{(1 + e^{-z})^{2}}\\frac{\\partial}{\\partial z}(-z)\\\\\n",
    "    &= \\frac{e^{-z}}{(1 + e^{-z})^{2}}\\\\\n",
    "    &= \\frac{e^{-z}}{1 + e^{-z}} \\frac{1}{1 + e^{-z}}\\\\\n",
    "    &= \\left(\n",
    "        \\frac{1 + e^{-z}}{1 + e^{-z}} -\n",
    "        \\frac{1}{1 + e^{-z}} \n",
    "    \\right)\\frac{1}{(1 + e^{-z})}\\\\\n",
    "    &= (1 - \\sigma(z))\\sigma(z)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Derivative of linear combination $z$ with respect to weight $w_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial z}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\sum^D_{j=1}w_j \\times x_j\\\\\n",
    "    &= x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Derivative of loss function $L$ with respect to weight $w_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_i} &= \n",
    "    \\frac{\\partial z}{\\partial w_i}\n",
    "    \\frac{\\partial \\sigma(z)}{\\partial z}\n",
    "    \\frac{\\partial L}{\\partial \\sigma(z)}\\\\ \n",
    "    &= x_i \\left[(1-\\sigma(z))\\sigma(z)\\right]\\left[\\frac{1-y}{1-\\sigma(z)}-\\frac{y}{\\sigma(z)}\\right]\\\\\n",
    "    &= x_i [(1-y)\\sigma(z) - (1-\\sigma(z))y] \\\\\n",
    "    &= x_i [\\sigma(z)-y\\sigma(z) - y + y\\sigma(z)] \\\\\n",
    "    &= x_i [\\sigma(z)- y] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
