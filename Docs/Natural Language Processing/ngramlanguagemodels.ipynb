{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imperial-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-doctor",
   "metadata": {},
   "source": [
    "---\n",
    "slug: \"/blog/ngramlanguagemodels\"\n",
    "date: \"2021-04-28\"\n",
    "title: \"N-Gram Language Models\"\n",
    "category: \"3 Natural Language Processing\"\n",
    "order: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-tracker",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-doctrine",
   "metadata": {},
   "source": [
    "A language model predicts a probability distribution of language.\n",
    "A simple and popular modeling approach to language modeling is the N-gram language model.\n",
    "The N-gram language model will be described in this post, in the context of two possible applications: text generation as well as text classification (spam detection).\n",
    "This post will only be covering N-gram language models at the word level.\n",
    "It should be noted that N-gram models can also be developed at the character or syllable level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-calendar",
   "metadata": {},
   "source": [
    "### N-gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-offer",
   "metadata": {},
   "source": [
    "An N-gram language model is a model that predicts the probability of a sequence based on the product of each word and the preceding words in the sequence to that word. \n",
    "The N-gram language model relies on the Markov assumption, which is that a current state only depends upon a limited number of prior states.\n",
    "\n",
    "The equations below display how to calculate the probability of a sequence given a corpus, as well as how to calculate the probability of an N-gram sequence ending in a particular word given a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-respondent",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    C(s) &\\rightarrow \\text{ Count of sequence in corpus}\\\\\n",
    "    N &\\rightarrow \\text{ Size of sequence}\\\\\n",
    "    n &\\rightarrow \\text{ Size of Ngram}\\\\\n",
    "    P(w_{1:N}) &= \\prod^N_{i=1} P(w_i|w_{i-n+1:i-1})\\\\\n",
    "    &\\propto \\sum^N_{i=1} \\log P(w_i|w_{i-n+1:i-1})\\\\\n",
    "    P(w_i|w_{i-n+1:i-1}) &= \\frac{C(w_{i-n+1:i-1}, w_i)}{C(w_{i-n+1:i-1})}\\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-enzyme",
   "metadata": {},
   "source": [
    "The above approach to calculating sequence probabilities will have issues when dealing with words that have never been seen before. \n",
    "A general approach to dealing with unseen words is to assign some small probability to unseen events.\n",
    "This approach is called smoothing. \n",
    "One particular kind of smoothing, called Laplace smoothing assigns a constant to all ngram counts.\n",
    "The sequence probability calculation adjusted using Laplace smoothing is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-detective",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    V &\\rightarrow \\text{ Size of vocabulary}\\\\\n",
    "    P(w_i|w_{i-n+1:i-1}) &= \\frac{1 + C(w_{i-n+1:i-1}, w_i)}{V + C(w_{i-n+1:i-1})}\\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-gates",
   "metadata": {},
   "source": [
    "Another form of smoothing, known as Good-Turing smoothing, takes into account the frequency of words with one less number of occurrences when calculating a probability for a word.\n",
    "The sequence probability calculation adjusted using Good-Turing smoothing is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-eligibility",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    N_r &\\rightarrow \\text{ Count of N-grams that appear $r$ times}\\\\\n",
    "    c_*(w_i) &= (C(w_{i-n+1:i-1}, w_i) + 1) \\frac{N_{C(w_{i-n+1:i-1}, w_i) + 1}}{N_{C(w_{i-n+1:i-1}, w_i)}} \\\\\n",
    "    P(w_i|w_{i-n+1:i-1}) &= \\frac{c_*(w_i)}{\\sum^{\\infty}_{r=1} N_r}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-camcorder",
   "metadata": {},
   "source": [
    "Perplexity is an internal measurment used to measure the power of a language model.\n",
    "The perplexity of a test set given a language model is the inverse probability of the test set, scaled by the number of words.\n",
    "The lower the perplexity, the higher the conditional probability of the word sequence.\n",
    "The perplexity measure equation is displayed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-elimination",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "p(W) &= \\sqrt[N]{\\prod^{N}_{i=1} \\frac{1}{P(w_i|w_{i-n+1:i-1})} }\\\\\n",
    "&= \\left[\\prod^{N}_{i=1} \\frac{1}{P(w_i|w_{i-n+1:i-1})}\\right]^{\\frac{1}{N}}\\\\\n",
    "&= \\left[\\prod^{N}_{i=1} P(w_i|w_{i-n+1:i-1})\\right]^{-\\frac{1}{N}}\\\\\n",
    "&= e^{\\ln  \\left[\\prod^{N}_{i=1} P(w_i|w_{i-n+1:i-1})\\right]^{-\\frac{1}{N}}}\\\\\n",
    "&= e^{ -\\frac{1}{N} \\ln  \\left[\\prod^{N}_{i=1} P(w_i|w_{i-n+1:i-1})\\right]}\\\\\n",
    "&= e^{ -\\frac{1}{N} \\left[\\sum^{N}_{i=1} \\ln P(w_i|w_{i-n+1:i-1})\\right]}\\\\\n",
    "&= \\exp\\left(-\\frac{1}{N} \\sum^{N}_{i=1} \\ln P(w_i|w_{i-n+1:i-1})\\right)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-europe",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-warrior",
   "metadata": {},
   "source": [
    "Code for an NGram language model that uses Good-Turing smoothing when calculating probabilities is displayed below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subtle-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NGramLanguageModel:\n",
    "\n",
    "\n",
    "    def __init__(self, n: int, pad: str=\"<PAD>\") -> None:\n",
    "        \"\"\" Instantiate NGram language model\n",
    "\n",
    "        Args:\n",
    "            n: size of NGrams\n",
    "            pad: string to use as padding\n",
    "        \"\"\"\n",
    "        self.vocab = set([])\n",
    "        self.ngrams = {}\n",
    "        self.totals = {}\n",
    "        self.pad = pad\n",
    "        self.n = n\n",
    "\n",
    "\n",
    "    def parse(self, line: str) -> List[str]:\n",
    "        \"\"\" Parse string and turn it into list of tokens, removing all\n",
    "            non-alphanumeric characters and splitting by space\n",
    "\n",
    "        Args:\n",
    "            line: string to be parsed\n",
    "\n",
    "        Returns:\n",
    "            list of parsed tokens\n",
    "        \"\"\"\n",
    "        line = re.sub(\"[^a-z\\s]\", \"\", line.lower().strip())\n",
    "        tokens = [t for t in line.split(' ') if t != '']\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def add(self, line: str) -> None:\n",
    "        \"\"\" Add line to language model\n",
    "\n",
    "        Args:\n",
    "            line: string to be added to language model\n",
    "        \"\"\"\n",
    "        seq = [self.pad for i in range(self.n)]\n",
    "        tokens = self.parse(line)\n",
    "\n",
    "        # Split list of tokens into NGrams and add to model\n",
    "        for i in range(len(tokens)):\n",
    "            seq = seq[1:] + [tokens[i]]                \n",
    "            tseq = tuple(seq[:-1])\n",
    "\n",
    "            if tseq in self.ngrams:\n",
    "\n",
    "                if tokens[i] in self.ngrams[tseq]: \n",
    "                    self.ngrams[tseq][tokens[i]] += 1\n",
    "\n",
    "                else: \n",
    "                    self.ngrams[tseq][tokens[i]] = 1\n",
    "\n",
    "            else: \n",
    "                self.ngrams[tseq] = { tokens[i]: 1 }\n",
    "\n",
    "            self.vocab.add(tokens[i])\n",
    "\n",
    "\n",
    "    def calcSmoothingCounts(self) -> None:\n",
    "        \"\"\" After all lines are added, this method can be called to generate\n",
    "            counts required for Good-Turing smoothing\n",
    "        \"\"\"\n",
    "\n",
    "        self.totalCounts = 0\n",
    "        self.counts = {} \n",
    "\n",
    "        for ngram in tqdm(self.ngrams):\n",
    "            for token in self.ngrams[ngram]:\n",
    "                c = self.ngrams[ngram][token]\n",
    "                self.counts[c] = self.counts[c] + 1 if c in self.counts else 1\n",
    "                self.totalCounts += 1\n",
    "\n",
    "\n",
    "    def prob(self, sequence: List[str]) -> float:\n",
    "        \"\"\" Calculate probability of sequence being produced by language model,\n",
    "            smoothed using Good-Turing smoothing\n",
    "\n",
    "        Args: \n",
    "            sequence: sequence as a list of tokens\n",
    "\n",
    "        Returns:\n",
    "            probability of language model producing sequence                \n",
    "        \"\"\"\n",
    "\n",
    "        tseq = tuple(sequence[:-1])\n",
    "\n",
    "        c = 0\n",
    "        if tseq in self.ngrams and \\\n",
    "           sequence[-1] in self.ngrams[tseq]:\n",
    "            c += self.ngrams[tseq][sequence[-1]]           \n",
    "\n",
    "        ncn = self.counts[c+1] if c+1 in self.counts else 0\n",
    "        ncd = self.counts[c] if c in self.counts else 0\n",
    "        n = ncn / ncd if ncd != 0 else 0\n",
    "\n",
    "        cstar = (c + 1) * n\n",
    "        return cstar / self.totalCounts\n",
    "\n",
    "\n",
    "    def perplexity(self, dataset: List[str]) -> float:\n",
    "        \"\"\" Calculate preplexity of dataset with regard to model\n",
    "\n",
    "        Args:\n",
    "            dataset: list of string sequences in testing dataset\n",
    "\n",
    "        Returns:\n",
    "            perplexity score                \n",
    "        \"\"\"\n",
    "\n",
    "        perp = 0; N = 0;\n",
    "        for line in dataset:\n",
    "\n",
    "            seq = [self.pad for i in range(self.n)]\n",
    "            tokens = self.parse(line)\n",
    "            N += len(tokens)\n",
    "\n",
    "            for i in range(len(tokens)):\n",
    "                seq = seq[1:] + [tokens[i]]                \n",
    "                prob = self.prob(seq)\n",
    "                perp += np.log(prob) if prob != 0 else 0\n",
    "\n",
    "        perp = np.exp((-1/N) * perp)\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-oakland",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-diamond",
   "metadata": {},
   "source": [
    "- Jurafsky, Daniel, and James H. Martin. *Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition*. Pearson, 2020.\n",
    "- Russell, Stuart J., et al. *Artificial Intelligence: A Modern Approach*. 3rd ed, Prentice Hall, 2010."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
